{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZHbaTAkJiZR",
        "outputId": "e1a56688-5457-44d9-ad2a-ed877cd08149"
      },
      "outputs": [],
      "source": [
        "#!pip install ucimlrepo\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# fetch dataset\n",
        "covertype = fetch_ucirepo(id=31)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = covertype.data.features\n",
        "y = covertype.data.targets\n",
        "\n",
        "df_combined = X.copy()\n",
        "df_combined['Cover_Type'] = y['Cover_Type']\n",
        "\n",
        "# metadata\n",
        "print(covertype.metadata)\n",
        "\n",
        "# variable information\n",
        "print(covertype.variables)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S_Ze9NTrKUw"
      },
      "source": [
        "# Check-in Week 8/9/10\n",
        "\n",
        "Here we will use a neural network for multiclass classification. The response variable here is the forest cover type\n",
        "\n",
        "For this dataset, we will use a normal deep neural network\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfVgOy-ms5tu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Check for GPU availability and set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Extremely simple model architecture\n",
        "model = nn.Sequential()\n",
        "\n",
        "# Input layer\n",
        "model.add_module(\"Input\", nn.Linear(in_features = X.shape[1], out_features = 1024))\n",
        "\n",
        "# Hidden layers\n",
        "model.add_module(\"Hidden Layer 1\", nn.Linear(in_features=1024, out_features=1024))\n",
        "model.add_module(\"Activation 1\", nn.Sigmoid())\n",
        "model.add_module(\"Hidden Layer 2\", nn.Linear(in_features=1024, out_features=1024))\n",
        "model.add_module(\"Activation 2\", nn.Sigmoid())\n",
        "\n",
        "# Output layer\n",
        "model.add_module(\"Output\", nn.Linear(in_features=1024, out_features = len(np.unique(y))))\n",
        "\n",
        "# Move model to GPU if available\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Vu9G4BZFq99"
      },
      "outputs": [],
      "source": [
        "batch_size = 100\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Shift the y labels to start from 0 (if this doesn't happen, PyTorch can't train)\n",
        "y = y - 1\n",
        "\n",
        "# Convert Trees Pandas DataFrame to Torch Tensors\n",
        "X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y.to_numpy().flatten(), dtype=torch.long)\n",
        "\n",
        "# Split dataset into training, validation and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_tensor, y_tensor, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "# Convert to DataLoader for batching\n",
        "train_dl = DataLoader(train_dataset, batch_size =batch_size, shuffle=True)\n",
        "val_dl = DataLoader(val_dataset, batch_size = batch_size, shuffle=False)\n",
        "test_dl = DataLoader(test_dataset, batch_size = batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXPCbUY8FyCV",
        "outputId": "036a0472-6d9d-4b1d-b8cd-665a16e2ab61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 accuracy: 0.4804 val_accuracy: 0.4882\n",
            "Epoch 2 accuracy: 0.4938 val_accuracy: 0.4907\n",
            "Epoch 3 accuracy: 0.5010 val_accuracy: 0.5131\n",
            "Epoch 4 accuracy: 0.4982 val_accuracy: 0.4888\n",
            "Epoch 5 accuracy: 0.4937 val_accuracy: 0.4833\n",
            "Epoch 6 accuracy: 0.4935 val_accuracy: 0.4811\n",
            "Epoch 7 accuracy: 0.5012 val_accuracy: 0.5013\n",
            "Epoch 8 accuracy: 0.5012 val_accuracy: 0.4939\n",
            "Epoch 9 accuracy: 0.5025 val_accuracy: 0.5011\n",
            "Epoch 10 accuracy: 0.4930 val_accuracy: 0.4879\n",
            "Epoch 11 accuracy: 0.5004 val_accuracy: 0.5083\n",
            "Epoch 12 accuracy: 0.5017 val_accuracy: 0.4958\n",
            "Epoch 13 accuracy: 0.4927 val_accuracy: 0.4964\n",
            "Epoch 14 accuracy: 0.4962 val_accuracy: 0.4965\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 30\n",
        "\n",
        "# We will use the cross entropy loss function for multiclass\n",
        "loss_function = nn.CrossEntropyLoss()\\\n",
        "\n",
        "# Stochastic gradient descent with momentum as optimizer with lr and momentum from hyperparameter tuning\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Our training function (just the same thing from the MNIST notebook)\n",
        "def train(model, num_epochs, train_dl, valid_dl, device, loss_function, optimizer):\n",
        "    loss_hist_train = [0] * num_epochs\n",
        "    accuracy_hist_train = [0] * num_epochs\n",
        "    loss_hist_valid = [0] * num_epochs\n",
        "    accuracy_hist_valid = [0] * num_epochs\n",
        "\n",
        "    # Form\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for x_batch, y_batch in train_dl:\n",
        "            # Move batches to GPU if available\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            pred = model(x_batch)\n",
        "            loss = loss_function(pred, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            loss_hist_train[epoch] += loss.item()*y_batch.size(0)\n",
        "            is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
        "            accuracy_hist_train[epoch] += is_correct.sum().cpu()\n",
        "\n",
        "        loss_hist_train[epoch] /= len(train_dl.dataset)\n",
        "        accuracy_hist_train[epoch] /= len(train_dl.dataset)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in valid_dl:\n",
        "                # Move batches to GPU if available\n",
        "                x_batch = x_batch.to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "                pred = model(x_batch)\n",
        "                loss = loss_function(pred, y_batch)\n",
        "                loss_hist_valid[epoch] += loss.item() * y_batch.size(0)\n",
        "                is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
        "                accuracy_hist_valid[epoch] += is_correct.sum().cpu()\n",
        "\n",
        "        loss_hist_valid[epoch] /= len(valid_dl.dataset)\n",
        "        accuracy_hist_valid[epoch] /= len(valid_dl.dataset)\n",
        "\n",
        "        print(f'Epoch {epoch+1} accuracy: {accuracy_hist_train[epoch]:.4f} val_accuracy: {accuracy_hist_valid[epoch]:.4f}')\n",
        "    return loss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid\n",
        "\n",
        "torch.manual_seed(1)\n",
        "hist_train_validation = train(model, num_epochs, train_dl, val_dl, device, loss_function, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1Y9OqBwpiu3"
      },
      "source": [
        "For our neural network training, we used crossentropyloss as the standard loss function as it combines logsoftmax to convert our logits into probabilities and measures how well the predicted probabilities match the true class labels.\n",
        "\n",
        "Since we have 7 forest cover types, using CrossEntropyLoss penalizes confident wrong predictions more heavily than uncertain ones and we can see that the loss decreases as the epochs increase.\n",
        "\n",
        "For our accuracy, we need it to detect overfitting by tracking it separately on our training and validation sets.\n",
        "\n",
        "For our learning rate, we decided with learning rate of 0.01 being the best with fast convergence, stable training epochs, and a solid final validation accuracy.\n",
        "\n",
        "\n",
        "\n",
        "For our hyperparameter tuning, we chose SGD from class with momentum to get convergence and was a solid choice for neural network training. For our batch size, we chose 100 as we wanted to have faster updates with generalization and 30 epochs to see model behavior after convergence which typically happens around 20 epochs without overfitting.\n",
        "\n",
        "To complete everything, we split it into a training of 70, validation of 15, and test of 15 using PyTorch's default initialization. Our forward pass computes the predictions and CrossEntropyLoss calculates the loss and our backward pass computing the gradient with SGD as our optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gf-sETUJtjsc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "U0i7P_0uB2Kj",
        "vg8ifPSNTVBE",
        "ncgeXIpSqvBZ",
        "pcSD4aTfFgj7",
        "BTwnFEgdNnHG",
        "CZY1BuQwY6fK"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
